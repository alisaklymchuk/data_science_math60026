{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Prerequisites\n",
        "\n",
        "- Basic familiarity with [Numpy](https://numpy.org/doc/stable/user/quickstart.html)\n",
        "- Basic familiarity with [Pyplot](https://matplotlib.org/stable/tutorials/introductory/pyplot.html)"
      ],
      "metadata": {
        "id": "HZEtNp_x0frf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Outline\n",
        "\n",
        "<a name=\"outline\"></a>\n",
        "\n",
        "- [Section 1](#section-1): Data pre-processing\n",
        "- [Section 2](#section-2): Principal Components Analysis (PCA)\n",
        "- [Section 3](#section-3): Non-Negative Matrix Factorisation (NMF)"
      ],
      "metadata": {
        "id": "A_350mf20jGm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emCvo0kWU6dN"
      },
      "source": [
        "# Linear dimensionality reduction\n",
        "\n",
        "\n",
        "The purpose of this notebook is to understand and implement two linear dimensionality reduction methods:\n",
        "\n",
        "1.   Principle Component Analysis (PCA)\n",
        "2.   Non-Negative Matrix Factorisation (NMF)\n",
        "\n",
        "\n",
        "The main idea of these methods is to approximate our data by a linear combination of a few components, which span the space for a low-dimensional representation of the data. Such representation is often useful to visualise and inspect the structure of the data in an easier way, allowing us to better understand their properties. Here we illustrate the application of PCA and NMF to a dataset of hand-written digits (MNIST)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYwtD2z9U4Oh"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy import linalg\n",
        "\n",
        "# Initial global configuration for matplotlib\n",
        "SMALL_SIZE = 12\n",
        "MEDIUM_SIZE = 16\n",
        "BIGGER_SIZE = 20\n",
        "\n",
        "plt.rc('font', size=MEDIUM_SIZE)          # controls default text sizes\n",
        "plt.rc('axes', titlesize=MEDIUM_SIZE)     # fontsize of the axes title\n",
        "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
        "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
        "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
        "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
        "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"section-1\"></a>\n",
        "# Section 1: Data pre-processing ([index](#outline))"
      ],
      "metadata": {
        "id": "FiXVoEgQ0wKj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEDuVSbJZccc"
      },
      "source": [
        "Let's first load the data. We will use the MNIST dataset which we obtained from `sklearn.datasets.fetch_openml`, but which you can load from the `mnist_data.csv` file. Similarly, the labels for the images can be found in `mnist_labels.csv`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## EDIT HERE\n",
        "PATH = ''\n",
        "labels = pd.read_csv('mnist_labels.csv').to_numpy()\n",
        "images = pd.read_csv('mnist_data.csv').to_numpy()\n",
        "print(labels.shape)\n",
        "print(images.shape)"
      ],
      "metadata": {
        "id": "25OrsrcZWwar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We plot one of the images, to do this we need to reshape the data."
      ],
      "metadata": {
        "id": "Kr8MF09lVmKp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNg8MaHLZ6Ur"
      },
      "outputs": [],
      "source": [
        "# Plotting the second image\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.xlabel('Pixel Index', size=14)\n",
        "plt.ylabel('Pixel Index', size=14)\n",
        "plt.title('Raw image', size=14)\n",
        "plt.imshow(images[1].reshape(28,28), cmap='gray')\n",
        "plt.colorbar();"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now pre-process the data."
      ],
      "metadata": {
        "id": "9CxBAmvPx342"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(images.shape)\n",
        "print(images.max())\n",
        "print(images.min())"
      ],
      "metadata": {
        "id": "fI3LsngNxlYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oN7fm0KZZ-BS"
      },
      "source": [
        "We need to standardise the data. The steps below ensure that the pixels will have zero mean and unitary variance across images. As some pixels are the same for all images (e.g. the corners), we need to be careful to avoid diving my zero."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fBMihh7asJe"
      },
      "outputs": [],
      "source": [
        "def standardise(X):\n",
        "    \"\"\"\n",
        "    Standardise features.\n",
        "\n",
        "    Parameters:\n",
        "        X (np.array): Feature matrix.\n",
        "\n",
        "    Returns:\n",
        "        X_std (np.array): Standardised feature matrix\n",
        "    \"\"\"\n",
        "\n",
        "    mu = np.mean(X, axis=0, keepdims=True)\n",
        "    sigma = np.std(X, axis=0, keepdims=True)\n",
        "    sigma[sigma == 0] = 1 # to avoid division by zero\n",
        "    X_std = (X - mu) / sigma\n",
        "\n",
        "    return X_std"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For PCA, we will only consider the first 500 images."
      ],
      "metadata": {
        "id": "OYcIk1UW0cdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_std = standardise(images[:500, :])"
      ],
      "metadata": {
        "id": "BotnpJC0yaXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kG-FDX04bW_-"
      },
      "outputs": [],
      "source": [
        "# Plotting the second image (now standardised)\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.xlabel('Pixel Index', size=14)\n",
        "plt.ylabel('Pixel Index', size=14)\n",
        "plt.title('Standardised image', size=14)\n",
        "plt.imshow(X_std[1].reshape(28,28), cmap='gray')\n",
        "plt.colorbar();"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now verify that a given pixel across all the images has zero mean and unitary standard deviation."
      ],
      "metadata": {
        "id": "43m1i0iX09EI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gg08VBEAbW_-"
      },
      "outputs": [],
      "source": [
        "rng = np.random.default_rng(0)\n",
        "random_pixel = rng.integers(0, 784)\n",
        "np.testing.assert_allclose(np.std(X_std[:, random_pixel]), 1)\n",
        "# Note that assert_allclose considers relative difference (which involves division)\n",
        "# we can't assert closeness to 0, so we need to add a small constant to both sides\n",
        "np.testing.assert_allclose(np.mean(X_std[:, random_pixel] + 0.00001), 0.00001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSCbEI9AZBYs"
      },
      "source": [
        "<a name=\"section-2\"></a>\n",
        "# Section 2: Principal Components Analysis (PCA) ([index](#outline))\n",
        "To perform PCA on\n",
        "a dataset $\\mathbf{X}$ for $m$ principal components,\n",
        "we need to perform the following steps:\n",
        "\n",
        "1. Compute the covariance matrix,  $\\mathbf C$.\n",
        "1. Find eigenvalues and corresponding eigenvectors for the covariance matrix, $\\mathbf C$.\n",
        "3.  Sort the eigenvalues and their corresponding eigenvectors in order of descending magnitude (of the eignenvalues).\n",
        "4.  Compute the projection onto the spaced spanned by the first $m$ eigenvectors.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now define the functions `covariance_matrix` and `pca_function` to implement PCA."
      ],
      "metadata": {
        "id": "JgNH8Fcl3Ulc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4YJZmlzdN4N"
      },
      "outputs": [],
      "source": [
        "## EDIT HERE\n",
        "def covariance_matrix(X):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "      X (np.ndarray): Samples matrix, shape (N, p).\n",
        "\n",
        "    Returns:\n",
        "     The covariance matrix, shape (p, p).\n",
        "\n",
        "    \"\"\"\n",
        "    return # <-- SOLUTION\n",
        "\n",
        "\n",
        "def pca_function(X, m):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "      X (np.ndarray): Samples matrix, shape (N, p).\n",
        "      m (int): Number of principal components (m<=p).\n",
        "\n",
        "    Returns:\n",
        "      X_pca (np.ndarray): Projected data, shape (N, m).\n",
        "      eigenvectors (np.ndarray): First m eigenvectors of C, shape (p, m).\n",
        "      eigenvalues (np.ndarray): First m eigenvalues of C, shape (m, ).\n",
        "\n",
        "    \"\"\"\n",
        "    # Computing the covariance matrix\n",
        "    C = covariance_matrix(X)\n",
        "\n",
        "    # Computing the eigendecomposition using the np.linalg.eigh function\n",
        "    eigen =  # <-- EDIT THIS LINE\n",
        "    # extract the m largest eigenvalues\n",
        "    eigenvalues = # <-- EDIT THIS LINE\n",
        "    # we need to account for numerical error in calculating the eigenvalues\n",
        "    threshold = 1e-10\n",
        "    eigenvalues[np.abs(eigenvalues) < threshold] = 0\n",
        "    # and the eigenvectors corresponding to the m largest eigenvalyes\n",
        "    eigenvectors =  # <-- EDIT THIS LINE\n",
        "\n",
        "    # Project the data onto the directions of eigenvectors\n",
        "    X_pca = # <-- EDIT THIS LINE\n",
        "\n",
        "    return X_pca, eigenvectors, eigenvalues"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idrpUm4gmsU7"
      },
      "source": [
        "Test the PCA function for three principal components ($m=3$)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNPDJWWci-Uw"
      },
      "outputs": [],
      "source": [
        "m = 3 # the number of principal components\n",
        "X_pca, eigenvectors, eigenvalues = pca_function(X_std, m)\n",
        "\n",
        "np.testing.assert_allclose(eigenvalues, np.array([45.281472, 30.465387, 27.128733]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpqS5vaTbW_-"
      },
      "source": [
        "Now consider a larger number such as $m=100$. Plot the spectrum, *i.e.*, the histogram of the eigenvalues of $\\mathbf{C}$ with the `density` parameter set to `True`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVzsGqCFUJI4"
      },
      "outputs": [],
      "source": [
        "m = 100\n",
        "X_pca, eigenvectors, eigenvalues = pca_function(X_std, m)\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.hist(eigenvalues, density=True)\n",
        "plt.xlabel('Eigenvalues')\n",
        "plt.ylabel('Density')\n",
        "plt.title('Spectrum of $C$')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iM3PS9bZbW_-"
      },
      "source": [
        "If we look at the spectrum, the presence of large values indicates there are directions with large variance *i.e.* signal is present in the data.\n",
        "\n",
        "If we **randomly permute** the pixels of each image, which is equivalent to 'randomising' the data, the signal, and therefore, the large eigenvalues disappear. Reshuffle every column separately, using `rng` below (not `np.random`) to remove correlations between pixels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHGX-jHFbW_-"
      },
      "outputs": [],
      "source": [
        "# EDIT HERE\n",
        "X_std_random = np.copy(X_std)\n",
        "n, p = X_std.shape\n",
        "\n",
        "# <-- SOLUTION\n",
        "# <-- SOLUTION\n",
        "# <-- SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now we plot the spectrum of the randomised data:"
      ],
      "metadata": {
        "id": "87ERTuR9CItW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNmBvkEmbW__"
      },
      "outputs": [],
      "source": [
        "X_std_random = standardise(X_std_random)\n",
        "X_pca_random, eigenvectors_random, eigenvalues_random = pca_function(X_std_random, m)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.hist(eigenvalues_random, density=True);\n",
        "plt.xlabel('Eigenvalues')\n",
        "plt.ylabel('Density')\n",
        "plt.title('Spectrum of $C_{rand}$')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxLIjWQ9bW__"
      },
      "source": [
        "**Question:** what can you say about the previous plot? Compare it with the case in which the columns were not randomly permuted ($m=100$)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c28wTKEsw3RF"
      },
      "source": [
        "Let's go back to the original case with $m=100$ and analyse what fraction of the overall variance is explained by the components. Edit this cell to calculate the total variance of $X$ (as the sum of the absolute values of the eigenvalues). Plot the explained proportion of variance for the first $100$ principal components. Your code should pass the included test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMbkOYDgbW__"
      },
      "outputs": [],
      "source": [
        "# EDIT HERE\n",
        "C = covariance_matrix(X_std)\n",
        "all_eigenvalues, _ = # <-- EDIT THIS LINE\n",
        "total_variance =  # <-- EDIT THIS LINE\n",
        "\n",
        "explained_variances = # <-- EDIT THIS LINE\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(np.arange(100), explained_variances)\n",
        "plt.axhline(y = 1, color = 'r', linestyle = '-', label='Full variance')\n",
        "plt.axhline(y = 0.8, color = 'k', linestyle = '--', label='80% of variance')\n",
        "plt.xlabel('Eigenvalue index')\n",
        "plt.ylabel('Proportion of explained variance')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "np.testing.assert_allclose(explained_variances[64], 0.8033379096824053)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOGDU1wlbW__"
      },
      "source": [
        "Complete the cell below to find the number of components that allows us to **explain at least 80%** of the total variance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "piEqZtZjbW__"
      },
      "outputs": [],
      "source": [
        "# EDIT HERE\n",
        "opt_k = # <-- EDIT THIS LINE\n",
        "print(opt_k)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBzJjmUkbW__"
      },
      "source": [
        "Let's now try and understand what these different principle components represent for a given image. Plot the first 20 eigenvectors as images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VbjHyn2bW__"
      },
      "outputs": [],
      "source": [
        "# EDIT HERE\n",
        "# <-- SOLUTION\n",
        "# <-- SOLUTION\n",
        "\n",
        "# <-- SOLUTION\n",
        "# <-- SOLUTION\n",
        "# <-- SOLUTION\n",
        "# <-- SOLUTION\n",
        "# <-- SOLUTION\n",
        "# <-- SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wo-M9rFPbW__"
      },
      "source": [
        "Using `opt_k` principal components, approximate the 6th image (setting `index_image` equal to `5`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Bz816nnbW__"
      },
      "outputs": [],
      "source": [
        "# EDIT HERE\n",
        "index_image = 5\n",
        "\n",
        "# represent each image as a linear combination of principal components\n",
        "approximate_image = # <-- SOLUTION\n",
        "# <-- SOLUTION\n",
        "# <-- SOLUTION\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(10,10))\n",
        "ax[0].imshow(X_std[index_image].reshape([28,28]))\n",
        "ax[0].set_title('Original Image')\n",
        "ax[0].set_xlabel('Pixel Index')\n",
        "ax[0].set_ylabel('Pixel Index')\n",
        "ax[1].imshow(approximate_image.reshape([28,28]))\n",
        "ax[1].set_xlabel('Pixel Index')\n",
        "ax[1].set_ylabel('Pixel Index')\n",
        "ax[1].set_title('Approximated Image');"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that the general shape of the two is present, including those points that are brightest on the original image (the end of the two)."
      ],
      "metadata": {
        "id": "9Dz-KmvEIXyR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhNqNVrgbW__"
      },
      "source": [
        "**Exercise:** compute the approximated images using the $m$ components that account for $70%$ and $90\\%$ of the total variance. What is the difference?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLj-yLiWbW__"
      },
      "source": [
        "From the original `images` array consisting of $5000$ images, create an array of the images of the digits 5 and 7 only.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EDIT HERE\n",
        "ind_5 = # <-- EDIT THIS LINE\n",
        "ind_7 =  # <-- EDIT THIS LINE\n",
        "X_5 = # <-- EDIT THIS LINE\n",
        "X_7 =  # <-- EDIT THIS LINE\n",
        "X_subset = # <-- EDIT THIS LINE\n",
        "n_5 = len(ind_5)\n",
        "n_7 = len(ind_7)\n",
        "n_datapoints = n_5 + n_7\n",
        "X_subset_std = standardise(X_subset)"
      ],
      "metadata": {
        "id": "vo9rGf2cJah7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we find the first two principal components of this new dataset and plot the data points in a 2D space based on their projections onto eigenvectors 1 and 2 (`PC1` and `PC2`)."
      ],
      "metadata": {
        "id": "_MDr6Hc9JYMj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXyBFJifbW__"
      },
      "outputs": [],
      "source": [
        "m = 2\n",
        "X_subset_pca, eigenvectors, eigenvalues = pca_function(X_subset_std, m)\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.scatter(X_subset_pca[:n_5, 0], X_subset_pca[:n_5, 1], label='Digit 5')\n",
        "plt.scatter(X_subset_pca[n_5:n_datapoints, 0],\n",
        "            X_subset_pca[n_5:n_datapoints, 1], label='Digit 7')\n",
        "plt.legend()\n",
        "plt.xlabel('Projection onto PC1')\n",
        "plt.ylabel('Projection onto PC2')\n",
        "plt.title('Digits 5 and 7')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-VtcmlZbW__"
      },
      "source": [
        "**Exercise:**\n",
        "1. Define a function that calculates the error between the original and the approximated image. A standard choice is the Mean Squared Error (MSE).\n",
        "2. Calculate the MSE for all images.\n",
        "3. Plot the average MSE as a function of $m$. Can we use this plot to decide `opt_m`?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPuIZbEP7E5P",
        "tags": []
      },
      "source": [
        "<a name=\"section-3\"></a>\n",
        "# Section 3: Non-negative matrix factorisation (NMF) ([index](#outline))\n",
        "\n",
        "Now we will look at non-negative matrix factorisation. NMF is a matrix factorisation method where we constrain the matrices to have non-negative elements (while PCA produces components with elements that could be both positive and negative).\n",
        "\n",
        "NMF factors our $N\\times p$ data matrix $\\mathbf X$ into matrices with nonnegative elements $\\mathbf W$ ($N\\times r$) and $\\mathbf H$ ($r\\times p$), i.e. $\\mathbf X \\approx \\mathbf W\\mathbf H$. $\\mathbf W\\mathbf H$ is lower-rank ($r < p$), hence it gives a low-dimensional approximation of $\\mathbf X$.\n",
        "\n",
        "Note that for non-negative matrix factorisation we require the input matrix to be non-negative. However we still want to implement some form of standarisation so that features on different scales don't dominate the cost function. Therefore, we normalise each feature to fall between 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMXf2zJULcKk"
      },
      "outputs": [],
      "source": [
        "## EDIT HERE\n",
        "# glabally normalise to 0-1\n",
        "def normalize_nmf(X):\n",
        "    \"\"\"\n",
        "    Globally normalise features.\n",
        "\n",
        "    Parameters:\n",
        "        X (np.array): Feature matrix.\n",
        "\n",
        "    Returns:\n",
        "        X_norm (np.array): Normalised feature matrix\n",
        "    \"\"\"\n",
        "    X_norm = ## <-- EDIT THIS LINE\n",
        "    return X_norm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will consider, and normalise, the first $1000$ images."
      ],
      "metadata": {
        "id": "n4SojkRUlBur"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qceeUSAqsVQ"
      },
      "outputs": [],
      "source": [
        "n_datapoints = 1000\n",
        "X_norm = normalize_nmf(images[:n_datapoints] / 255.)\n",
        "X_norm.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mi0Kp6lqvpUh"
      },
      "source": [
        "As explained in the lecture notes, we find $\\mathbf{W}$ and $\\mathbf{H}$ as the local minima of cost functions that represent the quality of the approximation of $\\mathbf{X}$ by $\\mathbf{WH}$. We will see the implementation of the optimisation of the two main cost functions used for NMF.\n",
        "\n",
        "## Cost function: Euclidean distance\n",
        "\n",
        "The first is the square of the Euclidean distance between the data $\\mathbf{X}$ and the product $\\mathbf{WH}$:\n",
        "\n",
        "<center>\n",
        "$\n",
        "||\\mathbf{X} - \\mathbf{WH}||^2 = \\sum_{ij}(X_{i,j} - (\\mathbf{WH})_{ij})^2\n",
        "$\n",
        "</center>\n",
        "\n",
        "Local minima of this function are found by the Lee and Seung's multiplicative update rules:\n",
        "<center>\n",
        "$\n",
        "W_{ij}^{t+1}\\leftarrow W_{ij}^{t}\n",
        "\\frac{(\\mathbf{X}(\\mathbf{H}^{t})^T)_{ij}}{(\\mathbf{W}^t \\mathbf{H}^{t} (\\mathbf{H}^{t})^T)_{ij}}\n",
        "$\n",
        "</center>\n",
        "\n",
        "and\n",
        "\n",
        "<center>\n",
        "$\n",
        "H_{jk}^{t+1}\\leftarrow H_{jk}^{t}\n",
        "\\frac{(( \\mathbf{W}^{t+1})^T \\mathbf{X})_{jk}}{((\\mathbf{W}^{t+1})^T \\mathbf{W}^{t+1} \\mathbf{H}^t)_{jk}}\n",
        "$\n",
        "</center>\n",
        "\n",
        "\n",
        "which are repeated for several iterations $n$ until $\\mathbf{W}$ and $\\mathbf{H}$ converge. It is important to note that updates are done on an element by element basis and not by  matrix multiplication. In practice, a small positive scalar is added to the denominators in the above expression in order to avoid division by zero.\n",
        "\n",
        "Edit the function below to calculate the cost for a given factorisation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxjXmQzwLcWx"
      },
      "outputs": [],
      "source": [
        "## EDIT HERE\n",
        "def euclidean_cost(X, W, H):\n",
        "    \"\"\"\n",
        "    Computes the Euclidean NMF cost function.\n",
        "\n",
        "    Parameters:\n",
        "      X (np.ndarray): Data matrix, shape (N, p).\n",
        "      W (np.ndarray): Weight matrix, shape (N, r).\n",
        "      H (np.ndarray): Feature matrix, shape (r, p).\n",
        "\n",
        "    Returns:\n",
        "      cost_value (float): Value of the cost function.\n",
        "    \"\"\"\n",
        "\n",
        "    # Check that W and H can be multiplied together\n",
        "    assert W.shape[1] == H.shape[0], \"The inner dimensions of W and H do not match for multiplication.\"\n",
        "\n",
        "    # Test to see whether the size of X and the size of W times H matches\n",
        "    assert X.shape == (W.shape[0], H.shape[1]), \"The dimensions of X do not match the dimensions of WH.\"\n",
        "\n",
        "    # compute the difference between X and the matrix product of W and H\n",
        "    diff = ## <-- EDIT THIS LINE\n",
        "\n",
        "    # Compute the Euclidean distance-based cost function\n",
        "    cost_value = ## <-- EDIT THIS LINE\n",
        "\n",
        "    return cost_value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlvSAxgShslE"
      },
      "source": [
        "Now we will implement Lee and Seung's multiplicative update rule. This algorithm works by iteratively, updating matrix $\\mathbf{W}$ and then matrix $\\mathbf{H}$ until convergence.\n",
        "\n",
        "As with all iterative schemes, initial values for $\\mathbf{W}$ and $\\mathbf{H}$ are needed. Whilst many procedures exist to obtain efficient initial values that aim to (i) improve convergence time and (ii) reduce final cost value, we will simply take random matrices as initial matrices $\\mathbf{W}$ and $\\mathbf{H}$.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EDIT THIS FUNCTION\n",
        "def euclidean_NMF(X, r, rng, epsilon=0.0001, n_iters=500):\n",
        "  \"\"\"\n",
        "  Implements NMF with the euclidean cost function.\n",
        "\n",
        "  Parameters:\n",
        "    X (np.ndarray): Data matrix, shape (N, p).\n",
        "    r (int): Number of components.\n",
        "    rng (np.random.default_rng): Random number generator.\n",
        "    epsilon (float): Small positive scalar, default 0.0001.\n",
        "    n_iters (int): Number of iterations, default 500.\n",
        "\n",
        "  Returns:\n",
        "    W (np.ndarray): Weight matrix, shape (N, r).\n",
        "    H (np.ndarray): Feature matrix, shape (r, p).\n",
        "    cost_values (list): List of cost values for each iteration.\n",
        "  \"\"\"\n",
        "  # N x r components matrix, usually interpreted as the weights of each feature\n",
        "  W = rng.random((X.shape[0], r))\n",
        "  # r x P matrix interpreted as the basis set\n",
        "  H = rng.random((r, X.shape[1]))\n",
        "\n",
        "  # empty list\n",
        "  cost_values = []\n",
        "\n",
        "  # loop over the n iterations\n",
        "  for _ in range(n_iters):\n",
        "      # compute the update on W\n",
        "      W =  ## <-- EDIT THIS LINE\n",
        "      # compute the update on H\n",
        "      H =  ## <-- EDIT THIS LINE\n",
        "      # compute the cost and append to list\n",
        "      cost_values.append(euclidean_cost(X, W, H))\n",
        "\n",
        "  return W, H, cost_values"
      ],
      "metadata": {
        "id": "onMBnv-Zmpao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rng = np.random.default_rng(0)\n",
        "r = 2\n",
        "W, H, cost_values = euclidean_NMF(X_norm, r, rng)"
      ],
      "metadata": {
        "id": "EW196Vcom4n5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o44FxW6WZJ9f"
      },
      "outputs": [],
      "source": [
        "np.testing.assert_allclose(cost_values[-1], 44724.21957)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIfQhL7rv6pr"
      },
      "source": [
        "We should verify that we have converged to a solution by plotting the value of our cost function over the iterations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6tLWQaYSWW6"
      },
      "outputs": [],
      "source": [
        "# plotting the cost, to check convergence\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(cost_values)\n",
        "plt.xlabel('Number of iterations')\n",
        "plt.ylabel('Cost function')\n",
        "plt.title('Convergence')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that the algorithm has converged.\n",
        "\n",
        "We now have a look at the columns of $\\mathbf{W}$ and the rows of $\\mathbf{H}$. Each column $\\mathbf{W}_{[:, j]}$ contains information about the relationship between the samples and feature $j$. Therefore we colour point $(W_{i, 0},W_{i, 1})$ by the digit label of sample $i$, to give an indication of the information relating to digit label contained in features $0$ and $1$. As $\\mathbf{H}$ contains information about the $p$ features rather than the samples, we can't similarly colour these points."
      ],
      "metadata": {
        "id": "h6ICnZBHoBsk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labs = labels[:1000].reshape(-1)\n",
        "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
        "for i in range(10):\n",
        "    axs[0].scatter(W[labs == i, 0], W[labs == i, 1],  label=i)\n",
        "axs[0].set_xlabel('$W_{:, 0}$')\n",
        "axs[0].set_ylabel('$W_{:, 1}$')\n",
        "axs[0].set_title('Columns of W')\n",
        "axs[0].legend()\n",
        "axs[1].scatter(H[0, :], H[1, :])\n",
        "axs[1].set_xlabel('$H_{0, :}$')\n",
        "axs[1].set_ylabel('$H_{1, :}$')\n",
        "axs[1].set_title('Rows of H')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4MW45nYYpWdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And plot the histograms for the first column of $\\mathbf{W}$ and first row of $\\mathbf{H}$."
      ],
      "metadata": {
        "id": "eAU_FouR7HbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labs = labels[:1000]\n",
        "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
        "axs[0].hist(W[:,0], bins = 40)\n",
        "axs[0].set_xlabel('$W_{:, 0}$')\n",
        "axs[0].set_ylabel('Frequency')\n",
        "axs[0].set_title('Histogram of $W_{:, 0}$')\n",
        "axs[1].hist(H[0, :], bins = 40)\n",
        "axs[1].set_xlabel('$H_{0, :}$')\n",
        "axs[1].set_ylabel('Frequency')\n",
        "axs[1].set_title('Histogram of $H_{0, :}$')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Rb_Hube3sv0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyqVkNp3YcIH"
      },
      "source": [
        "We can observe that:\n",
        "\n",
        "*   these matrices have non-negative entries as we expected\n",
        "*   they are sparse (which is a desirable property!)\n",
        "*   the columns of $W$ seem to contain some information about the cluster the image belongs to.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHXIZARZTUL9"
      },
      "source": [
        "\n",
        "\n",
        "## Cost function: Divergence\n",
        "\n",
        "Next, we repeat the optimisation monitoring the trend of a different cost function, the divergence $D$ defined as:\n",
        "<center>\n",
        "$\n",
        "D(\\mathbf{X}||\\mathbf{WH}) = \\sum_{ij}\\left[X_{ij} \\log\\left(\\frac{X_{ij}}{(\\mathbf{WH})_{ij}}\\right) - X_{ij} +(\\mathbf{WH})_{ij}\\right]\n",
        "$\n",
        "</center>\n",
        "\n",
        "for which we are guaranteed to find local minima by slightly different multiplicative update rules:\n",
        "\n",
        "<center>\n",
        "$\n",
        "W_{ij}^{t+1}\\leftarrow W_{ij}^{t}\n",
        "\\frac{\\sum_k H_{jk}^tX_{ik} / (\\mathbf{W}^t\\mathbf{H}^{t})_{ik} }{\\sum_k H^{t}_{jk}}\n",
        "$\n",
        "</center>\n",
        "\n",
        "and\n",
        "\n",
        "<center>\n",
        "$\n",
        "H_{jk}^{t+1}\\leftarrow H_{jk}^{t}\n",
        "\\frac{ \\sum_i W^{t+1}_{ij}X_{ik} / (\\mathbf{W}^{t+1}\\mathbf{H}^t)_{ik}}{\\sum_i W^{t+1}_{ij}}\n",
        "$\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first define the cost function `divergence_cost` before writing a function to implement NMF with this particular cost function, `divergence_NMF`."
      ],
      "metadata": {
        "id": "UdHbmnYg6NYi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5rTCBiGqsVT"
      },
      "outputs": [],
      "source": [
        "## EDIT HERE\n",
        "def divergence_cost(X, W, H, epsilon=0.0001):\n",
        "    \"\"\"\n",
        "    Computes the divergence NMF cost function.\n",
        "\n",
        "    Parameters:\n",
        "      X (np.ndarray): Data matrix, shape (N, p).\n",
        "      W (np.ndarray): Weight matrix, shape (N, r).\n",
        "      H (np.ndarray): Feature matrix, shape (r, p).\n",
        "\n",
        "    Returns:\n",
        "      cost_value (float): Value of the cost function.\n",
        "    \"\"\"\n",
        "\n",
        "    # Check that W and H can be multiplied together\n",
        "    assert W.shape[1] == H.shape[0], \"The inner dimensions of W and H do not match for multiplication.\"\n",
        "\n",
        "    # Test to see whether the size of X and the size of W times H matches\n",
        "    assert X.shape == (W.shape[0], H.shape[1]), \"The dimensions of X do not match the dimensions of WH.\"\n",
        "\n",
        "    # compute the divergence term by term\n",
        "    ## we add a small epsilon to avoid ill-defined divisions/log\n",
        "    term = ## <-- EDIT THIS LINE\n",
        "\n",
        "    # sum over the terms\n",
        "    cost_value = ## <-- EDIT THIS LINE\n",
        "    return cost_value"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EDIT THIS FUNCTION\n",
        "def divergence_NMF(X, r, rng, epsilon=0.0001, n_iters=500):\n",
        "  \"\"\"\n",
        "  Implements NMF with the divergence cost function.\n",
        "\n",
        "  Parameters:\n",
        "    X (np.ndarray): Data matrix, shape (N, p).\n",
        "    r (int): Number of components.\n",
        "    rng (np.random.default_rng): Random number generator.\n",
        "    epsilon (float): Small positive scalar, default 0.0001.\n",
        "    n_iters (int): Number of iterations, default 500.\n",
        "\n",
        "  Returns:\n",
        "    W (np.ndarray): Weight matrix, shape (N, r).\n",
        "    H (np.ndarray): Feature matrix, shape (r, p).\n",
        "    cost_values (list): List of cost values for each iteration.\n",
        "  \"\"\"\n",
        "  # N x r components matrix, usually interpreted as the weights of each feature\n",
        "  W = rng.random((X.shape[0], r))\n",
        "  # r x P matrix interpreted as the basis set\n",
        "  H = rng.random((r, X.shape[1]))\n",
        "\n",
        "  # empty list\n",
        "  cost_values = []\n",
        "\n",
        "  # loop over the n iterations\n",
        "  for _ in range(n_iters):\n",
        "\n",
        "      # compute the update on W\n",
        "      W = ## <-- EDIT THIS LINE\n",
        "      # compute the update on H\n",
        "      H = ## <-- EDIT THIS LINE\n",
        "      # compute the cost and append to list\n",
        "      cost_values.append(divergence_cost(X, W, H))\n",
        "\n",
        "  return W, H, cost_values"
      ],
      "metadata": {
        "id": "7jsbeCuU2nQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We implement NMF with $r=2$."
      ],
      "metadata": {
        "id": "V4rOa7G86Z0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r = 2\n",
        "rng = np.random.default_rng(0)\n",
        "W, H, cost_values = divergence_NMF(X_norm, r, rng)"
      ],
      "metadata": {
        "id": "LgCzINRRu5AY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It should pass the following test:"
      ],
      "metadata": {
        "id": "Q-fbqcr_6gRc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rk4hiZPLY4JJ"
      },
      "outputs": [],
      "source": [
        "np.testing.assert_allclose(cost_values[-1], 85293.31274)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, we check convergence has occured:"
      ],
      "metadata": {
        "id": "K4UU4iJ26ljZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbXy3JCoas2f"
      },
      "outputs": [],
      "source": [
        "# plotting the cost, to check convergence\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(cost_values)\n",
        "plt.xlabel('Number of iterations')\n",
        "plt.ylabel('Cost function')\n",
        "plt.title('Convergence')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note as this is a different cost function to that used previously, the final values at convergence for the two algorithms are not something we can compare.\n",
        "\n",
        "We now produce the same plots for NMF with the divergence cost function as for the Euclidean cost funciton."
      ],
      "metadata": {
        "id": "C1D0SIpXvJ1v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFh8hNDwqsVU"
      },
      "outputs": [],
      "source": [
        "labs = labels[:1000].reshape(-1)\n",
        "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
        "for i in range(10):\n",
        "    axs[0].scatter(W[labs == i, 0], W[labs == i, 1],  label=i)\n",
        "axs[0].set_xlabel('$W_{:, 0}$')\n",
        "axs[0].set_ylabel('$W_{:, 1}$')\n",
        "axs[0].set_title('Columns of W')\n",
        "axs[0].legend()\n",
        "axs[1].scatter(H[0, :], H[1, :])\n",
        "axs[1].set_xlabel('$H_{0, :}$')\n",
        "axs[1].set_ylabel('$H_{1, :}$')\n",
        "axs[1].set_title('Rows of H')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofeTDntQqM7o"
      },
      "outputs": [],
      "source": [
        "labs = labels[:1000]\n",
        "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
        "axs[0].hist(W[:,0], bins = 40)\n",
        "axs[0].set_xlabel('$W_{:, 0}$')\n",
        "axs[0].set_ylabel('Frequency')\n",
        "axs[0].set_title('Histogram of $W_{:, 0}$')\n",
        "axs[1].hist(H[0, :], bins = 40)\n",
        "axs[1].set_xlabel('$H_{0, :}$')\n",
        "axs[1].set_ylabel('Frequency')\n",
        "axs[1].set_title('Histogram of $H_{0, :}$')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And we see similar trends *i.e.* non-negativity and sparseness.\n",
        "\n",
        "Finally, we can have a look at the difference components resulting from our NMF and see what information they might contain. Edit the following cell so that it produces two plots side by side, of the first and second rows of $H$ plotted as images."
      ],
      "metadata": {
        "id": "-D57czrD7tOO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4g8y-Idb5jE"
      },
      "outputs": [],
      "source": [
        "# EDIT THIS CODE\n",
        "# <-- SOLUTION\n",
        "# <-- SOLUTION\n",
        "\n",
        "# <-- SOLUTION\n",
        "# <-- SOLUTION\n",
        "# <-- SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faICOEZes5P0"
      },
      "source": [
        "We now implement NMF with another value of r."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSJV2UZhrlQx"
      },
      "outputs": [],
      "source": [
        "# choosing the number of dimensions (r) on which to project\n",
        "r = 10\n",
        "rng = np.random.default_rng(0)\n",
        "W, H, cost_values = euclidean_NMF(X_norm, r, rng)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It should pass the following test:"
      ],
      "metadata": {
        "id": "l7ArqpAf50IM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcYyQ4jsdL0J"
      },
      "outputs": [],
      "source": [
        "np.testing.assert_allclose(cost_values[-1], 29502.85551)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, we would like to see what the different components represent. Edit the following cell so that it produces two rows of 5 plots, with one image for each rows of $H$."
      ],
      "metadata": {
        "id": "zmYX-li5535M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tx5tfPmgrlQx"
      },
      "outputs": [],
      "source": [
        "# EDIT THIS CODE\n",
        "# <-- SOLUTION\n",
        "# <-- SOLUTION\n",
        "\n",
        "# <-- SOLUTION\n",
        "# <-- SOLUTION\n",
        "# <-- SOLUTION\n",
        "# <-- SOLUTION\n",
        "# <-- SOLUTION\n",
        "# <-- SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercises:**\n",
        "\n",
        "\n",
        "\n",
        "*   Implement this with sklearn and check that the method has been correctly implemented.\n",
        "    *   We expect this to be slightly different, the sklearn implementation relies on the optimisation of a slightly different cost function:\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html\n",
        "*   Check that the result of NMF is the same if you\n",
        "use another version of cost function, the one from the paper:\n",
        "https://www.nature.com/articles/44565.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "O3ljJusXcU_s"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d_mB9u-q4NIg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}