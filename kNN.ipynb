{"cells":[{"cell_type":"markdown","metadata":{"id":"5o4LqdKHIrIG"},"source":["#### Prerequisites\n","\n","- Basic familiarity with [Numpy](https://numpy.org/doc/stable/user/quickstart.html)\n","- Basic familiarity with [Pyplot](https://matplotlib.org/stable/tutorials/introductory/pyplot.html)"]},{"cell_type":"markdown","metadata":{"id":"DTqKQ9khIvvz"},"source":["<a name=\"outline\"></a>\n","\n","## Outline\n","\n","- [Section 1](#section-1): Intro to kNN\n","- [Section 2](#section-2): Classification with kNN\n","- [Section 3](#section-3): Hyperparameter Tuning for $k$ with _T-fold_ Cross Validation\n","- [Section 4](#section-4): Regression with kNN\n","- [Extra 1](#extra-1): Weighted kNN"]},{"cell_type":"markdown","source":["# _k_ nearest neighbours (kNN)\n","\n","The purpose of this notebook is to understand and implement the kNN algorithm, without using any package that has a complete kNN framework already implemented (e.g., scikit-learn)."],"metadata":{"id":"-gfuVgBzhpn6"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"nGLDpvh2JLD5"},"outputs":[],"source":["import numpy as np\n","from sklearn.datasets import make_classification, fetch_california_housing\n","import matplotlib.pyplot as plt\n","from matplotlib.colors import ListedColormap\n","\n","SMALL_SIZE = 12\n","MEDIUM_SIZE = 16\n","BIGGER_SIZE = 20\n","plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n","plt.rc('axes', titlesize=BIGGER_SIZE)     # fontsize of the axes title\n","plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n","plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n","plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n","plt.rc('legend', fontsize=MEDIUM_SIZE)    # legend fontsize\n","plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n","\n","# fixing random generator for reproducibility\n","rng = np.random.default_rng(0)"]},{"cell_type":"markdown","metadata":{"id":"aQYOryXBIuJG"},"source":["\n","\n","<a name=\"section-1\"></a>\n","\n","## Section 1: Intro to kNN [^](#outline)\n","\n","The kNN algorithm can be used both for classification and regression. Broadly speaking, it starts with calculating the distance of a given point $x$ to all other points in the data set. Then, it finds the _k_ nearest points closest to $x$, and assigns the new point $x$ to the majority class of the _k_ nearest points _(classification)_. So, for example, if two of the _k_=3 closest points to $x$ were red while one is blue, $x$ would be classified as red.\n","\n","On the other hand in _regression_, we see the labels as continuous variables and assign the label of a data point $x$ as the mean of the labels of its _k_ nearest neighbours."]},{"cell_type":"markdown","metadata":{"id":"g3NNOP8ckB6s"},"source":["Important things first: You already know that the kNN algorithm is based on computing distances between data points. So, let's start with defining a function that computes this distance. For simplicity, we will only work with **Euclidean distances** in this notebook, but any other distance function could be used interchangably.\n","\n","Implement in the following cell the Euclidean distance $d$, defined as\n","$$\n","d(\\boldsymbol x^{(i)}, \\boldsymbol x^{(j)}) = \\sqrt{\\sum_{m=1}^p{(x^{(i)}_m-x^{(j)}_m)^2}} \\, ,\n","$$\n","where $\\boldsymbol x^{(i)}$ and $\\boldsymbol x^{(j)}$ are the two points in our $p$-dimensional Euclidean space.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yjGhYvoVkBU8"},"outputs":[],"source":["## EDIT THIS FUNCTION\n","def euclidean_distance(x_i, x_j):\n","\n","    '''\n","    Argument:\n","    x_i: vector with shape (p,)\n","    x_j: matrix with shape (n, p)\n","\n","    Returns:\n","    d: distance between x_i and each row of x_j\n","    '''\n","\n","    assert x_j.shape[1] == x_i.shape[0]\n","\n","    return ...  ## <-- EDIT THIS LINE"]},{"cell_type":"markdown","metadata":{"id":"eQ_TLVhzlNph"},"source":["<a name=\"section-2\"></a>\n","\n","## Section 2: Classification with kNN [^](#outline)\n","\n","We start with using kNN for classification tasks, and create a dataset with sklearn's [`make_classification`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) function and standardise the variables in our data. Variable standardisation means subtracting from each variable its mean across samples (so the mean of the standardised variable is zero) and next dividing by the standard deviation across samples (so the standard deviation of the standardised variable is one)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vg3-00Wkn77v"},"outputs":[],"source":["# make a dataset with 3 classes, 2 features and 100 samples\n","X, y = make_classification(n_samples=100, n_features=2, n_redundant=0, n_informative=2, n_clusters_per_class=1, n_classes=3, random_state=15)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gO_zwEHgYWDR"},"outputs":[],"source":["## EDIT THIS FUNCTION\n","def standardise(X, X_train_=None):\n","    \"\"\"Standardise features.\n","\n","    Parameters:\n","        X (np.array): Feature matrix.\n","        X_train_ (np.array): An optional feature matrix to compute the statistics\n","            from before applying it to X. If None, just use X to compute the statistics.\n","\n","    Returns:\n","        X_std (np.array): Standardised feature matrix\n","    \"\"\"\n","    if X_train_ is None:\n","        X_train_ = X\n","\n","    mu = ...  ## <-- EDIT THIS LINE\n","    sigma = ...  ## <-- EDIT THIS LINE\n","    X_std = ...  ## <-- EDIT THIS LINE\n","    return X_std"]},{"cell_type":"markdown","metadata":{"id":"TL7c00_JpDtx"},"source":["As with any other supervised machine learning method, we create a training and test set to learn and evaluate our model, respectively."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BL-xX8gboj8z"},"outputs":[],"source":["# shuffling the rows in X and y\n","perm_ind = rng.permutation(len(y))\n","X = X[perm_ind]\n","y = y[perm_ind]\n","\n","# we split train to test as 70:30\n","split_rate = 0.7\n","X_train, X_test = np.split(X, [int(split_rate*(X.shape[0]))])\n","y_train, y_test = np.split(y, [int(split_rate*(y.shape[0]))])"]},{"cell_type":"code","source":["# standardize train and test data\n","X_test_std = standardise(X_test, X_train_=X_train)\n","X_train_std = standardise(X_train)"],"metadata":{"id":"SsCc1gtApTU4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Me2LXr200JjL"},"source":["We visualise the data set with points in the training set being fully coloured and points in the test being half-transparent."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2hM5H8Ui0Zqe"},"outputs":[],"source":["# define colormaps\n","cm = plt.cm.RdBu\n","cm_bright = ListedColormap(['blue', 'red', 'green'])\n","\n","# visual exploration\n","plt.figure()\n","plt.xlabel(r'$x^{(1)}$')\n","plt.ylabel(r'$x^{(2)}$')\n","plt.scatter(X_train_std[:, 0], X_train_std[:, 1], c=y_train, cmap=cm_bright)\n","plt.scatter(X_test_std[:, 0], X_test_std[:, 1], c=y_test, cmap=cm_bright, alpha=0.25)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"C0GCsan8q2Uv"},"source":["We try to find the _k_ nearest neighbours in our training set for every test data point. The majority of labels of the _k_ closest training points determines the label of the test point."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0waZKvI9oj5s"},"outputs":[],"source":["## EDIT THIS FUNCTION\n","def k_neighbours(X_train, X_test, k=5, return_distance=False):\n","  \"\"\"\n","  This function finds the k nearest neighbours in X_train for each point in X_test .\n","\n","  Argument:\n","  X_train: training data\n","  X_test: test data\n","  k: number of nearest neighbours\n","  return_distance: if True, return distances too\n","\n","  Returns:\n","  np.array(neigh_ind): array of indices of k nearest neighbours\n","\n","  \"\"\"\n","\n","  n_neighbours = k\n","  dist = []\n","  neigh_ind = []\n","\n","  # compute distance from each point x_test in X_test to all points in X_train (hint: use python's list comprehension)\n","  point_dist = ...  ## <-- EDIT THIS LINE\n","\n","  # determine which k training points are closest to each test point\n","  for row in point_dist:\n","      enum_neigh = enumerate(row)\n","      sorted_neigh = sorted(enum_neigh, key=lambda x: x[1])[:k]\n","\n","      ind_list = [tup[0] for tup in sorted_neigh]\n","      dist_list = [tup[1] for tup in sorted_neigh]\n","\n","      dist.append(dist_list)\n","      neigh_ind.append(ind_list)\n","\n","  # return distances together with indices of k nearest neighbours\n","  if return_distance:\n","      return np.array(dist), np.array(neigh_ind)\n","\n","  return np.array(neigh_ind)"]},{"cell_type":"markdown","metadata":{"id":"c23MEsfNt3KR"},"source":["Once we know which _k_ neighbours are closest to our test points, we can predict the labels of these test points.\n","\n","Our `predict` function determines how any point $x_\\text{test}$ in the test set is classified. Here, we only consider the case where each of the *k* neighbours contributes equally to the classification of $x_\\text{test}$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Ll7dfHtoj2S"},"outputs":[],"source":["## EDIT THIS FUNCTION\n","def predict(X_train, y_train, X_test, k=5):\n","  \"\"\"\n","  This function predicts labels of test data given training data and labels.\n","\n","  Argument:\n","  X_train: training data\n","  y_train: training labels\n","  X_test: test data\n","  k: number of nearest neighbours\n","\n","  Returns:\n","  y_pred: predicted labels for X_test\n","\n","  \"\"\"\n","  # each of the k neighbours contributes equally to the classification of any data point in X_test\n","  neighbours = k_neighbours(X_train, X_test, k=k)\n","  # count number of occurences of label with np.bincount and choose the label that has most with np.argmax (hint: use python's list comprehension)\n","  y_pred = ...  ## <-- EDIT THIS LINE\n","\n","  return y_pred"]},{"cell_type":"markdown","metadata":{"id":"_5tbmvo7x2QZ"},"source":["To evaluate the algorithm in a more principled way, we need to implement a function that computes the mean accuracy by counting how many of the test points have been classified correctly and dividing this number by the total number of data points in our test set.\n","\n","Again, we do this is in a pythonic way and call the previous `predict` function _within_ the next function `score`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"trOotUZWj-a0"},"outputs":[],"source":["## EDIT THIS FUNCTION\n","def score(X_train, y_train, X_test, y_test, k=5):\n","\n","  y_pred = ...  ## <-- EDIT THIS LINE\n","\n","  return float(sum(y_pred==y_test))/ float(len(y_test))"]},{"cell_type":"markdown","metadata":{"id":"77rOIU0V395Q"},"source":["It is quite common to print both the training and test set accuracies."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GAPceMtej-Yo"},"outputs":[],"source":["k = 8\n","print('Training set mean accuracy:', score(X_train_std, y_train, X_train_std, y_train, k=k))\n","print('Test set mean accuracy:', score(X_train_std, y_train, X_test_std, y_test, k=k))"]},{"cell_type":"markdown","metadata":{"id":"e2ZMAV0l4Pup"},"source":["#### Questions\n","1. Does the solution above look reasonable?\n","2. Play around with different values for _k_. How does it influence the classification mean accuracy?\n","3. Compare the training and test set accuracy. Is there a difference? If so, what does the difference tell you?\n","4. Choose different ratios for the split between training and test set, and re-run the entire algorithm. What can you learn from different ratios?\n","5. Considering an accuracy estimate on a test-split wich contains 30% of the dataset examples is 0.86, do we guarantee to obtain the same accuracy estimate when we apply our model on infinetely large unseen test examples, i.e. does the accuracy of your model on the test-split generalize well on unseen data? From this week's lecture notes, what would you suggest to improve our confidence in the accuracy estimate, so it is a closer estimate to the true accuracy when testing on unseen examples?\n","\n","<a name=\"section-3\"></a>\n","\n","### 3 Hyperparameter Tuning for $k$ with _T-fold_ Cross Validation [^](#outline)\n","\n","Let's consider a systematic way to help select the best $k$ (i.e. the hyperparameter of $k$-NN). In previous cells, we split our data into 70%:30% for training:test examples. Now we need to choose the best $k$, but without looking at the accuracy on the test set (which should be used at the end to assess the predictive power of the model with the chosen $k$). To this end, we perform $T$-fold cross validation, where, **importantly**, we don't evaluate the accuracy of the model using the same examples on which we trained our model, rather on a held out validation set. We can achieve this by running $T$ experiments, and in each one we use disjoint partitions for the training and accuracy examples. By averaging the accuracy estimates over the $T$ experiments, we get a more precise and reliable accuracy estimate than before. If we consider $T=3$ folds, then we can run the three experiments and evaluate the average accuracy as in the figure below:\n","\n","![cv](https://raw.githubusercontent.com/barahona-research-group/mfds-resources/main/images/cv1.png)\n","\n","\n","Finaly, we need to isolate a separate set, before using cross-validation for hyperparameter tuning, to test our model after selecting the best performing $k$ for $k$-NN, so we further consider the following partitioning, which we will need to adhere with throughout the future notebooks and courseworks (i.e. in the problems when we need to use cross-validation for hyperparameter tuning then test on completely unseen data that are not involved in hyperparamter tuning):\n","\n","![cv2](https://raw.githubusercontent.com/barahona-research-group/mfds-resources/main/images/cv2.png)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"V4zbDOxnI_Gb"},"source":["Let's consider a ratio 80:20 for training and test splits. Before splitting, it is good practice to reshuffle the data in case they were grouped by label/outcome in some way, in such a way that after splitting the training/validation/test data are as statistically equivalent as possible."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pp0HADhkI9fN"},"outputs":[],"source":["# shuffling the rows in X and y\n","perm_ind = rng.permutation(len(y))\n","X = X[perm_ind]\n","y = y[perm_ind]\n","\n","# we split train to test as 80:20\n","split_rate = 0.8\n","X_train, X_test = np.split(X, [int(split_rate*(X.shape[0]))])\n","y_train, y_test = np.split(y, [int(split_rate*(y.shape[0]))])"]},{"cell_type":"markdown","metadata":{"id":"fPozNwa2Jc1_"},"source":["Now let's partition our training split into 5-folds. We could store the corresponding indices only:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7opw8k71J3cE"},"outputs":[],"source":["# Now we have a list of five index arrays, each correspond to one of the five folds.\n","folds_indexes = np.split(np.arange(len(y_train)), 5)\n","folds_indexes"]},{"cell_type":"markdown","metadata":{"id":"fWhl2dxPLR2S"},"source":["Let's implement a function that evalutes the accuracy of model with a given $k$ by running $T$ experiments and returning the average evaluated accuracy."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pqwT9DI0LZxy"},"outputs":[],"source":["# EDIT THIS FUNCTION\n","def cross_validation_score(X_train, y_train, folds, k):\n","  scores = []\n","  for i in range(len(folds)):\n","    val_indexes = folds[i]\n","    train_indexes = list(set(range(y_train.shape[0])) - set(val_indexes))\n","\n","    X_train_i = X_train[train_indexes, :]\n","    y_train_i = y_train[train_indexes]\n","\n","    X_val_i = ...  ## <-- EDIT THIS LINE\n","    y_val_i = ...  ## <-- EDIT THIS LINE\n","\n","    # We standardise both training and validation sets\n","    X_train_i_std = ... ## <-- EDIT THIS LINE\n","    X_val_i_std = ... ## <-- EDIT THIS LINE\n","\n","    score_i = ...  ## <-- EDIT THIS LINE\n","    scores.append(score_i)\n","\n","  # Return the average score\n","  return ...  ## <-- EDIT THIS LINE"]},{"cell_type":"markdown","metadata":{"id":"XFEovVPRJ1cy"},"source":["Let's scan a range of $k$ in $[1, 30]$ and select the one with the best cross-validation accuracy."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jhijlRSCKzAA"},"outputs":[],"source":["def choose_best_k(X_train, y_train, folds, k_range):\n","  k_scores = np.zeros((len(k_range),))\n","\n","  for i, k in enumerate(k_range):\n","    k_scores[i] = cross_validation_score(X_train, y_train, folds, k)\n","    print(f'CV_ACC@k={k}: {k_scores[i]:.3f}')\n","\n","  best_k_index = np.argmax(k_scores)\n","  return k_scores, k_range[best_k_index]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GHs8nb0LISvD"},"outputs":[],"source":["k_scores, best_k = choose_best_k(X_train, y_train, folds_indexes, np.arange(1, 31))\n","\n","print('best_k:', best_k)"]},{"cell_type":"code","source":["#Plot the cross-validation scores as a function of k\n","\n","plt.figure(figsize=(10, 5))\n","plt.plot(np.arange(1, 31), k_scores)\n","plt.xlabel(r'Number of Nearest Neighbours $k$')\n","plt.ylabel(r'Fold-averaged validation Accuracy')\n","plt.vlines(x=best_k, ymin=min(k_scores), ymax=1, linestyles='dashed')\n","plt.xticks(np.arange(1, 31))\n","plt.title(r\"kNN 5-fold cross-validation for $k$\")\n","plt.show()"],"metadata":{"id":"ANr_QdJk4xzd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sqBBm7hcTPSQ"},"source":["Finally, let's evaluate the accuracy with the best k on on the unseen part, the test-split that we isolated earlier."]},{"cell_type":"code","source":["# Standardise data.\n","X_train_std = standardise(X_train)\n","X_test_std = standardise(X_test, X_train_ = X_train)"],"metadata":{"id":"YeKOBHDiQoD8"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EcUHi8L7TNZr"},"outputs":[],"source":["score(X_train_std, y_train, X_test_std, y_test, k=best_k)"]},{"cell_type":"markdown","metadata":{"id":"rINjT0aFpQ-x"},"source":["\n","<a name=\"section-4\"></a>\n","\n","## Section 4:  Regression with kNN [^](#outline)\n","\n","The kNN algorithm is mostly used for classification, but we can also utilise it for (non-linear) regression. Here, we calculate the label of every point in the test set as the mean of the _k_ nearest neighbours.\n","\n","We start with defining a training set with sklearn's California housing data set. Note that this data set has normally 8 features, but we only extract the first feature, which corresponds to the median income in the district. The label is the median house value in the district.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0sLrzaHvj-Vw"},"outputs":[],"source":["data = fetch_california_housing(return_X_y=True)\n","X = data[0][:,0].reshape((-1, 1))\n","y = data[1]"]},{"cell_type":"markdown","metadata":{"id":"HZSFFaom85XT"},"source":["As before, we first divide the data into training and test set:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IOUrZpEpj-T4"},"outputs":[],"source":["# shuffling the rows in X and y\n","perm_ind = rng.permutation(len(y))\n","X = X[perm_ind]\n","y = y[perm_ind]\n","\n","# we split train to test as 70:30\n","split_rate = 0.7\n","X_train, X_test = np.split(X, [int(split_rate*(X.shape[0]))])\n","y_train, y_test = np.split(y, [int(split_rate*(y.shape[0]))])"]},{"cell_type":"markdown","source":["**Note:** Sometimes fetch_california_housing does not work. Instead you can load california_housing_train.csv and california_housing_test.csv from the sample_data folder in this colab file. Then follow the code below."],"metadata":{"id":"YrmPnxxy8CPt"}},{"cell_type":"code","source":["import pandas as pd"],"metadata":{"id":"pXI5h-rO8B4Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train = pd.read_csv('sample_data/california_housing_train.csv').to_numpy()\n","test = pd.read_csv('sample_data/california_housing_train.csv').to_numpy()\n","\n","# Extract the median_income column, which is the second to last column.\n","X_train = train[:,-2]\n","X_test = test[:,-2]\n","\n","# Extract the median_house_value column, which is the last column.\n","y_train = train[:,-1]\n","y_test = test[:,-1]"],"metadata":{"id":"1pgZ4BEq8AoD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# standardize train and test data\n","X_test_std = standardise(X_test, X_train_=X_train)\n","X_train_std = standardise(X_train)"],"metadata":{"id":"OhLcjqp-8WYM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lyaH2SKx-qvz"},"source":["Let's plot it to get a sense how we can proceed. This time, we plot training examples in blue and test examples in red."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vzmty5H5-qVe"},"outputs":[],"source":["# visual exploration\n","plt.xlabel(r\"median income in '000 USD\")\n","plt.ylabel(r\"median house value in '00.000 USD\")\n","plt.scatter(X_train_std, y_train, c='green', label='train', alpha=0.75)\n","plt.scatter(X_test_std, y_test, c='red', label='test', alpha=0.25)\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"D_NiqpEXBkF9"},"source":["As before, we need to define a predicting function which we call `reg_predict`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9WrjhUkFj-Oj"},"outputs":[],"source":["## EDIT THIS FUNCTION\n","def reg_predict(X_train, y_train, X_test, k=20):\n","\n","  # each of the k neighbours contributes equally to the classification of any data point in X_test\n","  neighbours = k_neighbours(X_train, X_test, k=k)\n","  # compute mean over neighbours labels (hint: use python's list comprehension)\n","  y_pred = ...  ## <-- EDIT THIS LINE\n","\n","  return y_pred"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mwVChY_aETF6"},"outputs":[],"source":["# computing predictions... (takes a few minutes due to the high sample size)\n","k = 20\n","y_pred = reg_predict(X_train_std, y_train, X_test_std, k=k)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gq6CgjUrETDU"},"outputs":[],"source":["# ... and plotting them\n","plt.xlabel(r\"median income (standardised)\")\n","plt.ylabel(r\"median house value in '00.000 USD\")\n","#plt.scatter(X_train, y_train, c='blue', alpha=0.25)\n","plt.scatter(X_test_std, y_test, c='red', label='true', alpha=0.25)\n","plt.scatter(X_test_std, y_pred, c='yellow', label='predicted', alpha=0.25)\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"mbBUhK8NRjqG"},"source":["To determine how well the prediction was, let us determine the $R^2$ score. The labels of the test set will be called $y$ and the predictions on the test data $\\hat{y}$.\n","$$\n","R^2(y, \\hat{y}) = 1 - \\frac{\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2} \\, ,\n","$$\n","where $\\bar{y} = \\frac{1}{n} \\sum_{i=1}^n y_i$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UbhMVuCGRic0"},"outputs":[],"source":["## EDIT THIS FUNCTION\n","def r2_score(y_test, y_pred):\n","  numerator = ...  ## <-- EDIT THIS LINE\n","  y_avg = ...  ## <-- EDIT THIS LINE\n","  denominator = ...  ## <-- EDIT THIS LINE\n","  return 1 - numerator/denominator"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"faowTZRXTgG7"},"outputs":[],"source":["print(r'R2 score:', r2_score(y_test, y_pred))"]},{"cell_type":"markdown","source":["<a name=\"extra-1\"></a>\n","\n","## Extra: weighted kNN [^](#outline)\n","\n","\n","\n","In weighted kNN, observations closer to the data point are given more weight than observations further away. The prediction may then be done by appling a weighted average of the $k$-nearest neighbours, using $\\frac{1}{d(x^{(i)}, x^{(j)})}$ as the weight. The prediction for data point $x^{(i)}$ is then as follows:\n","\n","\\begin{align}\n","\\hat{y}_{i} = \\frac{1}{\\sum_{j=1}^{k}d(x^{(i)}, x^{(j)})} \\sum_{j=1}^{k}  \\frac{y^{(j)}}{d(x^{(i)}, x^{(j)})}.\n","\\end{align}"],"metadata":{"id":"LG1_VSaXIZ0r"}},{"cell_type":"code","source":["## EDIT THIS FUNCTION\n","def reg_predict_w(X_train, y_train, X_test, k=20):\n","\n","  # each of the k neighbours contributes equally to the classification of any data point in X_test\n","  distances, neighbours = k_neighbours(X_train, X_test, k=k, return_distance=True)\n","  # compute mean over neighbours labels (hint: use python's list comprehension)\n","  y_pred =  ## <-- SOLUTION\n","\n","  return y_pred"],"metadata":{"id":"jUvRclbMIZne"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["k = 20\n","y_pred = reg_predict_w(X_train_std, y_train, X_test_std, k=k) # Note: takes a while to run as before."],"metadata":{"id":"T0yMvGLuIfoe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ... and plotting them\n","plt.xlabel(r\"median income (standardised)\")\n","plt.ylabel(r\"median house value in '00.000 USD\")\n","plt.scatter(X_test_std, y_test, c='red', label='true', alpha=0.25)\n","plt.scatter(X_test_std, y_pred, c='yellow', label='predicted', alpha=0.25)\n","plt.legend()\n","plt.show()"],"metadata":{"id":"F0qXD0p1Ii5u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(r'R2 score:', r2_score(y_test, y_pred))"],"metadata":{"id":"OCDTbudrIm1u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EMFsnTNxQpGc"},"source":["#### Questions\n","1. Does the solution above look reasonable? What does your $R^2$ value tell you?\n","2. Play around with different values for _k_. How does it influence the regression?\n","3. Like we did in classification, excercise with implementing cross-validation to find the best performing k on the regression task.\n","4. Compare the training and test set accuracy. Is there a difference? If so, what does the difference tell you?\n","5. Choose different ratios for the split between training and test set, and re-run the entire algorithm. What can you learn from different ratios?\n","6. Can you replicate your results using [sklearn](https://scikit-learn.org/stable/modules/neighbors.html)?\n","7. Based on sklearn's documentation, can you see any differences in the algorithms that are implemented in sklearn?"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"nbformat":4,"nbformat_minor":0}