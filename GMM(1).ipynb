{"cells":[{"cell_type":"markdown","source":["#### Prerequisites\n","\n","- Basic familiarity with [Numpy](https://numpy.org/doc/stable/user/quickstart.html)\n","- Basic familiarity with [Pyplot](https://matplotlib.org/stable/tutorials/introductory/pyplot.html)"],"metadata":{"id":"zZwKGdzhEtj4"},"id":"zZwKGdzhEtj4"},{"cell_type":"markdown","source":["# Outline\n","\n","<a name=\"outline\"></a>\n","\n","- [Section 1](#section-1): Data generation\n","- [Section 2](#section-2): Gaussian Mixture Model\n","- [Section 3](#section-3): Fitting Gaussian Mixture Models using the EM algorithm\n","- [Section 4](#section-4): Clustering of breast cancer data"],"metadata":{"id":"9M6p6ED8Ew_j"},"id":"9M6p6ED8Ew_j"},{"cell_type":"markdown","metadata":{"id":"vv6UlAn8MShm"},"source":["# Clustering using Gaussian Mixture Models\n","In this notebook, we implemenet clustering via Gaussian Mixture Models. We will first generate some synthetic data and define a Gaussian Mixture Model class (`GMModel`) before using the expectation-maximisation algorithm to optimise the model parameter.\n","\n"],"id":"vv6UlAn8MShm"},{"cell_type":"code","execution_count":null,"metadata":{"id":"eo-tVHLoMShn"},"outputs":[],"source":["# import libraries\n","import numpy as np\n","import numpy.testing as npt\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import copy\n","from matplotlib.patches import Ellipse\n","import matplotlib.pyplot as plt\n","\n","# Initial global configuration for matplotlib\n","SMALL_SIZE = 12\n","MEDIUM_SIZE = 16\n","BIGGER_SIZE = 20\n","\n","plt.rc('font', size=MEDIUM_SIZE)          # controls default text sizes\n","plt.rc('axes', titlesize=BIGGER_SIZE)     # fontsize of the axes title\n","plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n","plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n","plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n","plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n","plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title"],"id":"eo-tVHLoMShn"},{"cell_type":"markdown","source":["<a name=\"section-1\"></a>\n","# Section 1: Data generation ([index](#outline))"],"metadata":{"id":"trxGW4dRLkBb"},"id":"trxGW4dRLkBb"},{"cell_type":"markdown","source":["Sampling Gaussian mixtures is relatively straightforward. We define a function `synthetic_data` to generate data from two mixtures with the given parameters."],"metadata":{"id":"ZeC66uBHFN-m"},"id":"ZeC66uBHFN-m"},{"cell_type":"code","source":["def synthetic_data(mean_a, mean_b, cov, n, rng):\n","    \"\"\"\n","    Generates synthetic data from a Gaussian Mixture Model with two mixtures.\n","\n","    Parameters:\n","      mean_a (np.ndarray): The mean of the first Gaussian distribution, shape (p,).\n","      mean_b (np.ndarray): The mean of the second Gaussian distribution, shape (p,).\n","      cov (np.ndarray): The covariance matrix of the Gaussian distributions, shape (p, p).\n","      n (int): The number of data points to generate.\n","      rng (np.random.Generator): A random number generator.\n","\n","    Returns:\n","      np.ndarray: An array of the generated data points, shape (N, p).\n","    \"\"\"\n","\n","    a = rng.multivariate_normal(mean_a, cov, size=n)\n","    b = rng.multivariate_normal(mean_b, cov, size=n)\n","    return np.vstack((a, b)) # combine"],"metadata":{"id":"VrDy2xHxG9h_"},"execution_count":null,"outputs":[],"id":"VrDy2xHxG9h_"},{"cell_type":"markdown","source":["We now generate some data."],"metadata":{"id":"LtNxuSn4LcLN"},"id":"LtNxuSn4LcLN"},{"cell_type":"code","source":["rng = np.random.default_rng(0) # Set seed for reproducibility.\n","\n","# Define covariance of Gaussians\n","cov = np.array([[6, -3], [-3, 3.5]])\n","\n","# Define means of Gaussians\n","mean_a = np.array([0, 0])\n","mean_b = np.array([10, 5])\n","\n","n = 50\n","data = synthetic_data(mean_a, mean_b, cov, n, rng)\n","mixture_id = np.concatenate((np.zeros(n), np.ones(n)))"],"metadata":{"id":"DKEbF0_dxdSk"},"id":"DKEbF0_dxdSk","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["When can then visualise the data we generated here below.\n","\n"],"metadata":{"id":"Z_V3fWE7xe49"},"id":"Z_V3fWE7xe49"},{"cell_type":"code","execution_count":null,"metadata":{"id":"uY953akpMSho"},"outputs":[],"source":["# Visualise the data\n","fig = plt.figure(figsize=(16, 8))\n","ax1, ax2 = fig.add_subplot(121), fig.add_subplot(122)\n","\n","ax1.scatter(data[:, 0], data[:, 1], alpha=0.7)\n","ax1.set_title('Input Data')\n","ax1.set_xticklabels([]); ax1.set_yticklabels([])\n","ax1.set_xlabel('X1'); ax1.set_ylabel('X2')\n","# We use cluster_assignment here only to visualise the ground-truth\n","# (the ideal clustering) in practice we don't have it and aim to it\n","ax2.scatter(data[:, 0], data[:, 1], alpha=0.7, c=mixture_id, cmap='viridis')\n","ax2.set_title('Ground-Truth Assignments')\n","ax2.set_xticklabels([]); ax2.set_yticklabels([])\n","ax2.set_xlabel('X1'); ax2.set_ylabel('X2')\n","\n","# Show the plot\n","plt.show()"],"id":"uY953akpMSho"},{"cell_type":"markdown","id":"23ae3081","metadata":{"id":"23ae3081"},"source":["<a name=\"section-2\"></a>\n","# Section 2: Gaussian Mixture Model ([index](#outline))\n","\n","Following the lecture notes, the Gaussian Mixture Model (GMM) is given by:\n","$$\n","P(\\mathbf{x}=\\mathbf{x}^{(i)}) = \\sum_{k=1}^K \\pi_k p_k(\\mathbf{x}^{(i)}|\\mathbf{\\theta})\\, .\n","$$\n","where $K$ is the number of clusters described as mixture components each of which are multivariate normal distributions:\n","$$\n","p_k(\\mathbf{x}|\\mathbf{\\theta}) = {\\displaystyle (2\\pi )^{-p/2}\\det({\\boldsymbol {\\Sigma }_k})^{-1/2}\\,\\exp \\left(-{\\frac {1}{2}}(\\mathbf {x} -{\\boldsymbol {\\mu }_k})^{\\!{\\mathsf {T}}}{\\boldsymbol {\\Sigma }_k}^{-1}(\\mathbf {x} -{\\boldsymbol {\\mu }_k})\\right),}\n","$$\n","where $\\boldsymbol{\\theta} = \\{\\pi_k,\\mathbf{\\mu}_k, \\mathbf{\\Sigma}_k \\}_{k=1,2,...,K}$ is the vector of parameters consiting of the mixture weights $\\pi_k$, mixture component means $\\boldsymbol{\\mu}_k$ and mixture component covariance matrices $\\boldsymbol{\\Sigma}_k$.\n","\n","We start by implementing a class `MultivariateNormal` which defines a multivariate normal distribution and contains a method `pdf`."]},{"cell_type":"code","source":["## EDIT THIS FUNCTION\n","class MultivariateNormal():\n","    \"\"\"\n","    Multivariate normal distribution.\n","\n","    Parameters:\n","      mean (np.ndarray): the mean of the distribution, shape (p,).\n","      cov (np.ndarray): the covariance matrix of the distribution, shape (p, p).\n","    \"\"\"\n","\n","    def __init__(self, mean, cov):\n","        \"\"\"Initialises the multivariate normal distribution.\"\"\"\n","        self.mean = mean\n","        self.cov = cov\n","        self.cov_inv = np.linalg.inv(cov)\n","        self.cov_det = np.linalg.det(cov)\n","\n","    def pdf(self, X):\n","        \"\"\"\n","        Computes the probability density function of the distribution.\n","\n","        Parameters:\n","            X (np.ndarray): the data points, shape (N, p).\n","\n","        Returns:\n","            np.ndarray: the probability density function values, shape (N,).\n","        \"\"\"\n","        # Tip: you need to be careful with the dimensions to ensure you output the shape you expect\n","        matrix_factor = ## <- EDIT HERE\n","        return np.exp(matrix_factor) * (2 * np.pi) ** (- self.p / 2) * (self.cov_det) ** (-0.5)"],"metadata":{"id":"AqR5QQYi7QmP"},"id":"AqR5QQYi7QmP","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This should pass the following simple test case."],"metadata":{"id":"9dlp2KzqP8rJ"},"id":"9dlp2KzqP8rJ"},{"cell_type":"code","source":["X_testing = np.array([[1.0, 2.0],\n","                      [3.0, -1.0],\n","                      [-3.0, 1.0]])\n","expected_output = np.array([0.022799327319919294, 0.006532116642342461, 0.006532116642342461])\n","npt.assert_allclose(MultivariateNormal(np.array([0,0]), 2*np.eye(2)).pdf(X_testing), expected_output)"],"metadata":{"id":"i53yZdOlP793"},"id":"i53yZdOlP793","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We now define a class for the GMM model.\n","\n","**Initialisation:**  The samples are randomly split into $K$  groups and the means of these groups are taken as the initial values of the means. The covariance matrix is initialised as the covariance matrix of the data."],"metadata":{"id":"IUTeBgGNORXq"},"id":"IUTeBgGNORXq"},{"cell_type":"code","execution_count":null,"id":"e4360476","metadata":{"id":"e4360476"},"outputs":[],"source":["class GMModel:\n","    \"\"\"\n","    Class to define Gaussian Mixture Model.\n","\n","    Parameters:\n","      X (np.ndarray): the samples array, shape (N, p).\n","      k (int): number of mixture components, i.e. number of clusters.\n","\n","    Attributes:\n","      pi (np.ndarray): mixture weights, shape(k,).\n","      weights (np.ndarray): mixture weights, shape (N, p).\n","      mu (list): mixture component means for each cluster.\n","      sigma (list): mixture component covariance matrix for each cluster.\n","    \"\"\"\n","\n","    def __init__(self, X, k, rng):\n","        \"\"\"Initialises parameters.\"\"\"\n","        self.k = k\n","\n","        # initial weights given to each cluster are stored in pi, pi_k\n","        self.pi = np.full(shape=self.k, fill_value=1/self.k)\n","\n","        # initial weights given to each data point wrt to each cluster or w_ik(theta)\n","        self.weights = np.full(shape=X.shape, fill_value=1/self.k)\n","\n","        # initial value of mean of k Gaussians\n","        indices = np.arange(X.shape[0])\n","        rng.shuffle(indices)\n","        random_samples = np.array_split(indices, self.k)\n","        self.mu = [X[row_index,:].mean(axis=0) for row_index in random_samples]\n","\n","        # initial value of covariance matrix of k Gaussians\n","        self.sigma = [np.cov(X.T) for _ in range(self.k)]\n","\n","    def copy(self):\n","        \"\"\"Return an isolated copy.\"\"\"\n","        return copy.deepcopy(self)"]},{"cell_type":"markdown","id":"b843c45b","metadata":{"id":"b843c45b"},"source":["We can perform 'soft' clustering of the data by calculating the cluster probabilities of the data:\n","$$r_{ik}(\\boldsymbol{\\theta})=P(z=k|\\mathbf{x}=\\mathbf{x}^{(i)},\\boldsymbol{\\theta}) = \\frac{\\pi_k p_k(\\mathbf{x}^{(i)}|\\boldsymbol{\\theta})}{\\sum_{k'=1}^K {\\pi_{k'} p_{k'}(\\mathbf{x}^{(i)}|\\boldsymbol{\\theta})}}$$\n","This denotes the probability of data point $i$ belonging to cluster $k$. Generally, this yields a distribution over each data point."]},{"cell_type":"code","execution_count":null,"id":"d9d80996","metadata":{"id":"d9d80996"},"outputs":[],"source":["## EDIT THIS FUNCTION\n","def cluster_probabilities(gmm, X):\n","    \"\"\"\n","    Predicts probability of each data point with respect to each cluster\n","\n","    Parameters:\n","        gmm (GMModel): the GMM model.\n","        X (np.ndarray): the samples array, shape (N, p).\n","\n","    Returns:\n","        probabilities (np.ndarray): probabilities of each data point w.r.t each cluster, shape (N, k)\n","    \"\"\"\n","\n","    # N: number of rows\n","    # P: number of columns of dataset X\n","    N, P = X.shape\n","\n","    # Compute a N*k matrix denoting likelihood belonging to each cluster\n","    likelihood = np.array([MultivariateNormal(mean=gmm.mu[i], cov=gmm.sigma[i]).pdf(X) for i in range(gmm.k)]).T\n","\n","    # Compute probabilities from equation above.\n","    numerator = # <-- EDIT HERE\n","    denominator = # <-- EDIT HERE\n","    probabilities = # <-- EDIT HERE\n","    return probabilities"]},{"cell_type":"markdown","source":["Let's make sure that we pass the following test cases."],"metadata":{"id":"EYWzZnw5fLU6"},"id":"EYWzZnw5fLU6"},{"cell_type":"code","source":["X_testing = np.array([[1.0, 2.0],\n","                      [3.0, -1.0],\n","                      [-3.0, 1.0]])\n","expected_output = np.array([[0.81757448, 0.18242552],\n","                            [0.18242552, 0.81757448],\n","                            [0.81757448, 0.18242552]])\n","rng = np.random.default_rng(0)\n","gmm_test = GMModel(X_testing, 2, rng)\n","npt.assert_allclose(cluster_probabilities(gmm_test, X_testing), expected_output)"],"metadata":{"id":"EX4dyEp9fKtN"},"id":"EX4dyEp9fKtN","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"123842d7","metadata":{"id":"123842d7"},"source":["For visualisation it is useful to present the results as hard clusters on the output. This is done through the argmax of the cluster distribution. We now define a function `predict` to implement this."]},{"cell_type":"code","execution_count":null,"id":"77e98950","metadata":{"id":"77e98950"},"outputs":[],"source":["## EDIT THIS FUNCTION\n","def predict(gmm, X):\n","    \"\"\"\n","    Performs hard clustering.\n","\n","    Parameters:\n","      gmm (GMModel): the GMM model.\n","      X (np.ndarray): the samples array, shape (N, p).\n","\n","    Returns:\n","      labels (np.ndarray): cluster assignments for each data point, shape (N,).\n","    \"\"\"\n","\n","    probabilities = cluster_probabilities(gmm, X)\n","    return  # <-- EDIT HERE"]},{"cell_type":"markdown","id":"07b152ba","metadata":{"id":"07b152ba"},"source":["Before trying this on our synthetic data, we define functions to help us visualise the mixture model. These are borrowed from the *Python Data Science Handbook*. See [here](https://jakevdp.github.io/PythonDataScienceHandbook/05.12-gaussian-mixtures.html).\n","\n","\n","We first define a function `draw_ellipse` which given  position and covariance parameters draws an ellipse."]},{"cell_type":"code","execution_count":null,"id":"9e1c7c85","metadata":{"id":"9e1c7c85"},"outputs":[],"source":["def draw_ellipse(position, covariance, ax=None, **kwargs):\n","    \"\"\"\n","    Draw an ellipse with a given position and covariance\n","\n","    Parameters:\n","      position:  Center of the ellipse, shape (2,).\n","      covariance: Covariance matrix of the ellipse, shape (2, 2).\n","      ax: Matplotlib axis to draw on\n","      **kwargs: Additional keyword arguments for the ellipse\n","    \"\"\"\n","\n","    ax = ax or plt.gca()\n","\n","    # Convert covariance to principal axes\n","    if covariance.shape == (2, 2):\n","        U, s, Vt = np.linalg.svd(covariance)\n","        angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))\n","        width, height = 2 * np.sqrt(s)\n","    else:\n","        angle = 0\n","        width, height = 2 * np.sqrt(covariance)\n","\n","    # Draw the Ellipse - with 3 contours\n","    for nsig in range(1, 4):\n","        ax.add_patch(Ellipse(position, nsig * width, nsig * height,\n","                             angle=angle, **kwargs))"]},{"cell_type":"markdown","source":["Secondly we define `gmm_plot` which takes a GMModel and a dataset. It predicts the mixture membership for the samples given the model, plotting the samples and ellipses representing the mixtures.\n","\n"],"metadata":{"id":"L5MkRbLe80GP"},"id":"L5MkRbLe80GP"},{"cell_type":"code","source":["def gmm_plot(gmm, X, ax=None, **kwargs):\n","    \"\"\"\n","    Parameters:\n","      gmm (GMModel): the GMM model.\n","      X (np.ndarray): the samples array, shape (N, p).\n","      ax: Matplotlib axis to draw on.\n","      **kwargs: Additional keyword arguments for the ellipse.\n","    \"\"\"\n","    plt.figure(figsize=(10, 6))\n","    ax = ax or plt.gca()\n","    labels = predict(gmm, X)\n","    ax.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis', zorder=2)\n","    ax.axis('equal')\n","\n","    w_factor = 0.2 / gmm.weights.max()\n","    for pos, covar, w in zip(gmm.mu, gmm.sigma, gmm.pi):\n","        # alpha sets the transparency of the ellipse\n","        draw_ellipse(pos, covar, alpha=w * w_factor)\n","\n","    ax.set_title('GMM Assignments')\n","    ax.set_xlabel('X1'); ax.set_ylabel('X2')\n","    ax"],"metadata":{"id":"2swXgT3w4-KK"},"id":"2swXgT3w4-KK","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"16f916c5","metadata":{"id":"16f916c5"},"source":["We can now cluster the synthetic data using a GMM model with randomly initialised parameters:"]},{"cell_type":"code","execution_count":null,"id":"057f2a10","metadata":{"id":"057f2a10"},"outputs":[],"source":["rng = np.random.default_rng(0)\n","gmm = GMModel(data, 2, rng)\n","gmm_plot(gmm, data)"]},{"cell_type":"markdown","source":["As expected, the two clusters obtained from the randomly initialised GMM do not match the ground truth clusters at all because we first need to learn the parameters."],"metadata":{"id":"CuwVaHGo1Jn0"},"id":"CuwVaHGo1Jn0"},{"cell_type":"markdown","id":"ce34d4c7","metadata":{"id":"ce34d4c7"},"source":["<a name=\"section-3\"></a>\n","# Section 3: Fitting Gaussian Mixture Models using the EM algorithm ([index](#outline))\n","\n","We employ the EM algorithm to fit the data. The algorithm iteratively updates parameters of the Gausian Mixture. The algorithm is guaranteed to improve (or at least not worsen) the marginal likelihood of the data. At iteration $t$ the algorithm performs following steps:\n","\n","\n","1. **Expectation step (E-step)**\n","The cluster probabilities are computed using the current parameter values, $\\boldsymbol{\\theta}^{(t)}$.\n","\n","\tThe weights are updated from the cluster probabilities via\n","$$w_{ik}(\\boldsymbol{\\theta}^{(t)})=\\frac{r_{ik}(\\boldsymbol{\\theta}^{(t)})}{\\sum_{i'} r_{i'k}(\\boldsymbol{\\theta}^{(t)})}$$\n","\n","2. **Maximisation step (M-step)**\n","The model's parameters for maximising the log-likelihood at the next iteration $(t+1)$ are found.\n","\n","\tThe mixture weights are updated via:\n","$$\n","\t\\pi_k^{(t+1)} = \\frac{1}{N}\\sum_{i=1}^N r_{ik}(\\boldsymbol{\\theta}^{(t)}),\n","$$\n","and the cluster means are computed using a weighted mean:\n","$$\t\\boldsymbol{\\mu}_k^{(t+1)} =\\sum_{i=1}^N w_{ik}(\\boldsymbol{\\theta}^{(t)}) \\mathbf{x}^{(i)},$$\n","and similarly the covariances are updated via:\n","$$\\boldsymbol{\\Sigma}_k^{(t+1)}= \\sum_{i=1}^N w_{ik}(\\boldsymbol{\\theta}^{(t)}) (\\mathbf{x}^{(i)}-\\boldsymbol{\\mu}_k^{(t+1)}) (\\mathbf{x}^{(i)}-\\boldsymbol{\\mu}_k^{(t+1)})^T.$$\n","\n","\n","\n","A single iteration of these steps is implemented in the following function:"]},{"cell_type":"code","execution_count":null,"id":"e8c95ea6","metadata":{"id":"e8c95ea6"},"outputs":[],"source":["## EDIT THIS FUNCTION\n","def gmm_fit_step(gmm, X):\n","    \"\"\"\n","    Performs an EM iteration, updating all parameters via and E and M-step.\n","\n","    Args:\n","        gmm (GMModel): the current GMM model.\n","        X (np.ndarray): the samples array, shape (N, p).\n","\n","    Returns:\n","        gmm (GMModel): an updated GMM model after applying the E-M steps.\n","    \"\"\"\n","    gmm = gmm.copy()\n","\n","    # E-Step: compute probabilities and update weights holding mu/sigma/pi constant\n","    probabilities =  # <-- EDIT HERE\n","    weights = # <-- EDIT HERE\n","\n","    # M-Step: update mu, sigma and pi holding probabilities and weights constant\n","    gmm.pi =  # <-- EDIT HERE\n","    for i in range(gmm.k):\n","        gmm.mu[i] =  # <-- EDIT HERE\n","        gmm.sigma[i] = # <-- EDIT HERE\n","\n","    return gmm"]},{"cell_type":"markdown","source":["Let's make sure we pass the following test case."],"metadata":{"id":"bs8sAiLPhAuQ"},"id":"bs8sAiLPhAuQ"},{"cell_type":"code","source":["rng = np.random.default_rng(0)\n","X_testing = np.array([[1.0, 2.0],\n","                      [3.0, -1.0],\n","                      [-3.0, 1.0]])\n","gmm_test = GMModel(X_testing, 2, rng)\n","gmm_test = gmm_fit_step(gmm_test, X_testing)\n","expected_mu = np.array([[-0.59852974,  1.24908109],\n","                        [ 1.76575382, -0.22859614]])\n","expected_sigma = [np.array([[ 5.04323241e+00, -3.30673111e-03],\n","                            [-3.30673111e-03,  7.89245088e-01]]),\n","                   np.array([[ 4.64786728, -1.82495164],\n","                             [-1.82495164,  1.41058613]])]\n","npt.assert_allclose(gmm_test.mu, expected_mu)\n","npt.assert_allclose(gmm_test.sigma, expected_sigma)"],"metadata":{"id":"7v7KrJkHhEEI"},"id":"7v7KrJkHhEEI","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"09944c39","metadata":{"id":"09944c39"},"source":["Implementing one EM iteration we see that it does not significantly improve the result:"]},{"cell_type":"code","execution_count":null,"id":"66916afe","metadata":{"id":"66916afe"},"outputs":[],"source":["gmm = gmm_fit_step(gmm, data)\n","gmm_plot(gmm, data)"]},{"cell_type":"markdown","id":"1a4611f0","metadata":{"id":"1a4611f0"},"source":["Let's see if we can observe some improvement after several iterations:"]},{"cell_type":"code","execution_count":null,"id":"d9475b36","metadata":{"id":"d9475b36"},"outputs":[],"source":["for _ in range(60):\n","    gmm = gmm_fit_step(gmm, data)\n","gmm_plot(gmm, data)"]},{"cell_type":"markdown","id":"67152480","metadata":{"id":"67152480"},"source":["<a name=\"section-3\"></a>\n","# Section 4: Clustering of breast cancer data ([index](#outline))\n","\n","In this final section we will again work with the [Breast Cancer Wisconsin (Diagnostic) Data Set](https://www.kaggle.com/uciml/breast-cancer-wisconsin-data), which you can download directly from Blackboard. The data set contains various aspects of cell nuclei of breast screening images of patients with _(malignant)_ and without _(benign)_ breast cancer. Our goal is to cluster the data without the knowledge of the tumor being malignant or benign.\n","\n","If you run this notebook locally on your machine, you will simply need to place the `csv` file in the same directory as this notebook.\n","If you run this notebook on Google Colab, you will need to use\n","\n","  `from google.colab import files`\n","\n","  `upload = files.upload()`\n","\n","and then upload it from your local downloads directory."]},{"cell_type":"code","source":["from google.colab import files\n","\n","upload = files.upload()"],"metadata":{"id":"lfVPa-xKjAYI"},"id":"lfVPa-xKjAYI","execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = pd.read_csv('data.csv')\n","# print shape and last 10 rows\n","print(data.shape)"],"metadata":{"id":"NhKQfWpXMIC0"},"id":"NhKQfWpXMIC0","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's have a look at the last ten rows to see what features the dataset describes."],"metadata":{"id":"IjdJoeaxMb2u"},"id":"IjdJoeaxMb2u"},{"cell_type":"code","source":["data.tail(10)"],"metadata":{"id":"jDjjftrtMQ1b"},"id":"jDjjftrtMQ1b","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We now do some pre-processing steps:"],"metadata":{"id":"J698LFgIMkbc"},"id":"J698LFgIMkbc"},{"cell_type":"code","source":["# drop last column (extra column added by pandas)\n","data = data.drop(data.columns[-1], axis=1)\n","# set column id as dataframe index, then drop that column\n","data = data.set_index(data['id']).drop(data.columns[0], axis=1)\n","\n","# convert categorical labels to numbers\n","diag_map = {'M': 1.0, 'B': 0}\n","data['diagnosis'] = data['diagnosis'].map(diag_map)\n","\n","# split target from features\n","y = np.asarray(data.loc[:, 'diagnosis'])\n","X = np.asarray(data.iloc[:, 1:])\n","\n","data.tail(10)"],"metadata":{"id":"Cu_MF2MjMWHk"},"id":"Cu_MF2MjMWHk","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We start by visualising the ground-truth labels with respect to two featurs, the Mean Radius and Mean Texture of the tumor."],"metadata":{"id":"lqUECa6r3qeU"},"id":"lqUECa6r3qeU"},{"cell_type":"code","source":["# plot for two features\n","fig = plt.figure(figsize=(10, 6))\n","labels = ['Malignant', 'Benign']\n","for i, c in enumerate(np.unique(y)):\n","    plt.scatter(X[:, 0][y==c], X[:, 1][y==c], label=labels[i])\n","plt.xlabel('Mean Radius')\n","plt.ylabel('Mean Texture')\n","plt.title('Ground truth of Diagnoses')\n","plt.xlim([5,30])\n","plt.legend()\n","plt.show()"],"metadata":{"id":"F03v3FhkUtcV"},"id":"F03v3FhkUtcV","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In the following, we apply GMM clustering to the data set only using these two features (Mean Radius and Mean Texture)."],"metadata":{"id":"k4ahAdV_4-kS"},"id":"k4ahAdV_4-kS"},{"cell_type":"code","execution_count":null,"id":"ada3074d","metadata":{"id":"ada3074d"},"outputs":[],"source":["# restrict features\n","X = X[:, :2]\n","\n","# initialise GMM model\n","rng = np.random.default_rng(0)\n","gmm2 = GMModel(X, 2, rng)\n","\n","# plot prediction\n","gmm_plot(gmm2, X)\n","plt.xlabel('Mean Radius')\n","plt.ylabel('Mean Texture')\n","plt.title('Clustering of Diagnoses')\n","plt.ylim([5, 30])\n","plt.show()"]},{"cell_type":"markdown","source":["After initialising our GMM model we can fit it to the data."],"metadata":{"id":"a8b3MWOI9k84"},"id":"a8b3MWOI9k84"},{"cell_type":"code","execution_count":null,"id":"6199ee0d","metadata":{"id":"6199ee0d"},"outputs":[],"source":["for _ in range(100):\n","    gmm2 = gmm_fit_step(gmm2, X)\n","gmm_plot(gmm2, X)\n","plt.xlabel('Mean Radius')\n","plt.ylabel('Mean Texture')\n","plt.title('Clustering of Diagnoses')\n","plt.show()"]},{"cell_type":"markdown","source":["Comparing this plot to that showing the ground truth of diagnosis, we see that we have largely recovered the structure of the two clusters. However, as expected it is less able to correctly separate those samples which fall in a region populated by both clusters."],"metadata":{"id":"NA90TTsPgZNb"},"id":"NA90TTsPgZNb"},{"cell_type":"markdown","id":"97750383","metadata":{"id":"97750383"},"source":["**Questions:**\n","\n","* How do you know that the EM algorithm converged?\n","* How could the clustering of the cancer data set be improved?\n","* Can you quantify the uncertainty of cluster assignments for the cancer data set?\n","* Can you think of caveats when optimising hyperparameters of GMMs?\n","* What are suitable criteria for clustering quality using mixture models?"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}